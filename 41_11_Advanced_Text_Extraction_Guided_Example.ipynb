{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "41.11  Advanced Text Extraction - Guided Example.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or9Lvp__LY0p",
        "colab_type": "text"
      },
      "source": [
        "# Drill\n",
        "\n",
        "Take the well-known 20 newsgroups dataset and use each of the methods on it. Your goal is to determine which method, if any, best reproduces the topics represented by the newsgroups. Write up a report where you evaluate each method in light of the 'ground truth'- the known source of each newsgroup post. Which works best, and why do you think this is the case?\n",
        "\n",
        "### Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHgh10jRLgRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avloLpy6LklZ",
        "colab_type": "text"
      },
      "source": [
        "### 20 Newsgroups Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfjftVbdLm45",
        "colab_type": "code",
        "outputId": "7557d4df-7271-4b61-ecc9-6967c729e8f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups = fetch_20newsgroups()\n",
        "\n",
        "from pprint import pprint\n",
        "pprint(list(newsgroups.target_names))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism',\n",
            " 'comp.graphics',\n",
            " 'comp.os.ms-windows.misc',\n",
            " 'comp.sys.ibm.pc.hardware',\n",
            " 'comp.sys.mac.hardware',\n",
            " 'comp.windows.x',\n",
            " 'misc.forsale',\n",
            " 'rec.autos',\n",
            " 'rec.motorcycles',\n",
            " 'rec.sport.baseball',\n",
            " 'rec.sport.hockey',\n",
            " 'sci.crypt',\n",
            " 'sci.electronics',\n",
            " 'sci.med',\n",
            " 'sci.space',\n",
            " 'soc.religion.christian',\n",
            " 'talk.politics.guns',\n",
            " 'talk.politics.mideast',\n",
            " 'talk.politics.misc',\n",
            " 'talk.religion.misc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnckR6bmL5cJ",
        "colab_type": "text"
      },
      "source": [
        "### Generating the tf-idf Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_sC1CnvLgu5",
        "colab_type": "code",
        "outputId": "e467b2da-8fd2-4721-e323-c27f8d09d23e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Processing the data.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#reading in the data, this time in the form of paragraphs\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups = fetch_20newsgroups()\n",
        "\n",
        "#processing\n",
        "newsgroups_paras=[]\n",
        "for paragraph in newsgroups:\n",
        "    para=paragraph[0]\n",
        "    #removing the double-dash from all words\n",
        "    para=[re.sub(r'--','',word) for word in para]\n",
        "    #Forming each paragraph into a string and adding it to the list of strings.\n",
        "    newsgroups_paras.append(' '.join(para))\n",
        "\n",
        "# Creating the tf-idf matrix.\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "newsgroups_paras_tfidf=vectorizer.fit_transform(newsgroups_paras)\n",
        "\n",
        "# Getting the word list.\n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "# Number of topics.\n",
        "ntopics=5\n",
        "\n",
        "# Linking words to topics\n",
        "def word_topic(tfidf, solution, wordlist):\n",
        "    \n",
        "    # Loading scores for each word on each topic/component.\n",
        "    words_by_topic=tfidf.T * solution\n",
        "\n",
        "    # Linking the loadings to the words in an easy-to-read way.\n",
        "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
        "    \n",
        "    return components\n",
        "\n",
        "# Extracts the top N words and their loadings for each topic.\n",
        "def top_words(components, n_top_words):\n",
        "    n_topics = range(components.shape[1])\n",
        "    index= np.repeat(n_topics, n_top_words, axis=0)\n",
        "    topwords=pd.Series(index=index)\n",
        "    for column in range(components.shape[1]):\n",
        "        # Sort the column so that highest loadings are at the top.\n",
        "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
        "        # Choose the N highest loadings.\n",
        "        chosen=sortedwords[:n_top_words]\n",
        "        # Combine loading and index into a string.\n",
        "        chosenlist=chosen.index +\" \"+round(chosen,2).map(str) \n",
        "        topwords.loc[column]=chosenlist\n",
        "    return(topwords)\n",
        "\n",
        "# Number of words to look at for each topic.\n",
        "n_top_words = 10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-35cbc69891ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Creating the tf-idf matrix.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mnewsgroups_paras_tfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsgroups_paras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Getting the word list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \"\"\"\n\u001b[1;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    990\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CngQTxJhMReI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}