{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "It's time to revisit your classifier from the previous assignment. Using the evaluation techniques we've covered here, look at your classifier's performance in more detail. Then go back and iterate by engineering new features, removing poor features, or tuning parameters. Repeat this process until you have five different versions of your classifier. Once you've iterated, answer these questions to compare the performance of each:\n",
    "\n",
    "- Do any of your classifiers seem to overfit?\n",
    "- Which seem to perform the best? Why?\n",
    "- Which features seemed to be most impactful to performance?\n",
    "\n",
    "Write up your iterations and answers to the above questions in a few pages. Submit a link below and go over it with your mentor to see if they have any other ideas on how you could improve your classifier's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Amazon Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df = pd.read_excel('Amazon Dataframe.xlsx', delimiter='\\t', header=None)\n",
    "amazon_df.columns = ['Feedback', 'Sentiment']\n",
    "\n",
    "# Make all the feedback lowercase.\n",
    "amazon_df['Feedback'] = amazon_df['Feedback'].str.lower()\n",
    "\n",
    "# Strip out punctuation and numbers.\n",
    "amazon_df['Feedback'] = amazon_df['Feedback'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Add spaces before and after each entry in 'Feedback'.\n",
    "amazon_df['Feedback'] = ' ' + amazon_df['Feedback'] + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the        513\n",
       "i          316\n",
       "and        310\n",
       "it         281\n",
       "is         243\n",
       "a          218\n",
       "this       206\n",
       "to         196\n",
       "phone      162\n",
       "my         143\n",
       "for        121\n",
       "of         119\n",
       "not        117\n",
       "with       112\n",
       "very       103\n",
       "great       97\n",
       "was         90\n",
       "on          89\n",
       "in          88\n",
       "that        80\n",
       "good        75\n",
       "have        73\n",
       "you         68\n",
       "product     55\n",
       "quality     49\n",
       "had         48\n",
       "headset     47\n",
       "works       47\n",
       "but         46\n",
       "battery     45\n",
       "as          45\n",
       "its         43\n",
       "so          42\n",
       "are         42\n",
       "all         41\n",
       "use         41\n",
       "sound       41\n",
       "one         40\n",
       "well        38\n",
       "ear         35\n",
       "work        34\n",
       "has         34\n",
       "would       34\n",
       "from        33\n",
       "your        32\n",
       "dont        31\n",
       "like        30\n",
       "case        29\n",
       "if          29\n",
       "than        28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(' '.join(amazon_df['Feedback']).lower().split()).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Feature Engineering on the Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_keywords = ['good', 'excellent', 'great', 'love']\n",
    "\n",
    "for key in amazon_keywords:\n",
    "    amazon_df[str(key)] = amazon_df.Feedback.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Building the Training Data on the Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_data = amazon_df[amazon_keywords]\n",
    "amazon_target = amazon_df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Model on the Amazon Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in the Amazon dataframe out of a total 1000 points : 376\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(amazon_data, amazon_target)\n",
    "\n",
    "y_pred = mnb.predict(amazon_data)\n",
    "\n",
    "print('Number of mislabeled points in the Amazon dataframe out of a total {} points : {}'.format(\n",
    "    amazon_data.shape[0],\n",
    "    (amazon_target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to make five versions of this model and each one will be analyzed using: \n",
    "\n",
    "1. Success rate;\n",
    "2. Confusion matrix; and \n",
    "3. Cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the First Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Success Rate Evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 62.4 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_score(amazon_target, y_pred)\n",
    "print('The accuracy of the model is', accuracy_score(amazon_target, y_pred) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Confusion Matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[494,   6],\n",
       "       [370, 130]], dtype=int64)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(amazon_target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False Positive Count: six.\n",
    "- False Negative Count: 370.\n",
    "- Sensitivity: the model correctly identified 26% of the positives.\n",
    "- Specificity: 98.9% of negatives were correctly identified by the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Cross Validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69, 0.66, 0.66, 0.64, 0.63, 0.61, 0.61, 0.59, 0.59, 0.57])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb, amazon_data, amazon_target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Second Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Setting up the Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_amazon_df = amazon_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Looking at the Top 75 Most Used Words in the 'Feedback' Column_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 513,\n",
       " 'i': 316,\n",
       " 'and': 310,\n",
       " 'it': 281,\n",
       " 'is': 243,\n",
       " 'a': 218,\n",
       " 'this': 206,\n",
       " 'to': 196,\n",
       " 'phone': 162,\n",
       " 'my': 143,\n",
       " 'for': 121,\n",
       " 'of': 119,\n",
       " 'not': 117,\n",
       " 'with': 112,\n",
       " 'very': 103,\n",
       " 'great': 97,\n",
       " 'was': 90,\n",
       " 'on': 89,\n",
       " 'in': 88,\n",
       " 'that': 80,\n",
       " 'good': 75,\n",
       " 'have': 73,\n",
       " 'you': 68,\n",
       " 'product': 55,\n",
       " 'quality': 49,\n",
       " 'had': 48,\n",
       " 'headset': 47,\n",
       " 'works': 47,\n",
       " 'but': 46,\n",
       " 'battery': 45,\n",
       " 'as': 45,\n",
       " 'its': 43,\n",
       " 'so': 42,\n",
       " 'are': 42,\n",
       " 'all': 41,\n",
       " 'use': 41,\n",
       " 'sound': 41,\n",
       " 'one': 40,\n",
       " 'well': 38,\n",
       " 'ear': 35,\n",
       " 'work': 34,\n",
       " 'has': 34,\n",
       " 'would': 34,\n",
       " 'from': 33,\n",
       " 'your': 32,\n",
       " 'dont': 31,\n",
       " 'like': 30,\n",
       " 'case': 29,\n",
       " 'if': 29,\n",
       " 'than': 28,\n",
       " 'me': 28,\n",
       " 'ive': 28,\n",
       " 'price': 27,\n",
       " 'be': 27,\n",
       " 'after': 27,\n",
       " 'excellent': 27,\n",
       " 'time': 27,\n",
       " 'no': 26,\n",
       " 'up': 26,\n",
       " 'recommend': 26,\n",
       " 'does': 26,\n",
       " 'really': 26,\n",
       " 'im': 24,\n",
       " 'at': 24,\n",
       " 'service': 23,\n",
       " 'or': 23,\n",
       " 'best': 23,\n",
       " 'when': 22,\n",
       " 'only': 22,\n",
       " 'nice': 22,\n",
       " 'out': 22,\n",
       " 'get': 22,\n",
       " 'also': 22,\n",
       " 'too': 21,\n",
       " '2': 21}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_75_words = pd.Series(' '.join(second_amazon_df['Feedback']).lower().split()).value_counts()[:75]\n",
    "top_75_words.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Feature Engineering on the Second Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_amazon_keywords_list = ['good', 'excellent', 'great', 'love', 'nice', 'best', 'well', 'works', 'like', 'really']\n",
    "\n",
    "for key in second_amazon_keywords_list:\n",
    "    second_amazon_df[str(key)] = second_amazon_df.Feedback.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Building the Training Data on the Second Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_amazon_data = second_amazon_df[second_amazon_keywords_list]\n",
    "second_amazon_target = second_amazon_df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Running the Model on the Second Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in the Amazon dataframe out of a total 1000 points : 319\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(second_amazon_data, second_amazon_target)\n",
    "\n",
    "y_pred = mnb.predict(second_amazon_data)\n",
    "\n",
    "print('Number of mislabeled points in the Amazon dataframe out of a total {} points : {}'.format(\n",
    "    second_amazon_data.shape[0],\n",
    "    (second_amazon_target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Second Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Success Rate Evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the second model is 68.10000000000001 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_score(second_amazon_target, y_pred)\n",
    "print('The accuracy of the second model is', accuracy_score(second_amazon_target, y_pred) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Confusion Matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[491,   9],\n",
       "       [310, 190]], dtype=int64)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(second_amazon_target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False Positive Count: nine.\n",
    "- False Negative Count: 310.\n",
    "- Sensitivity: the model correctly identified 38% of the positives.\n",
    "- Specificity: 98.2% of negatives were correctly identified by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Cross Validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74, 0.71, 0.71, 0.7 , 0.7 , 0.61, 0.65, 0.64, 0.67, 0.61])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb, second_amazon_data, second_amazon_target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Third Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Setting up the Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_amazon_df = amazon_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_negative_reviews_df = third_amazon_df[third_amazon_df['Sentiment'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 237,\n",
       " 'and': 188,\n",
       " 'i': 154,\n",
       " 'is': 141,\n",
       " 'it': 128,\n",
       " 'a': 105,\n",
       " 'this': 105,\n",
       " 'great': 92,\n",
       " 'to': 86,\n",
       " 'phone': 86,\n",
       " 'my': 72,\n",
       " 'very': 69,\n",
       " 'for': 66,\n",
       " 'with': 65,\n",
       " 'good': 62,\n",
       " 'of': 49,\n",
       " 'works': 46,\n",
       " 'on': 44,\n",
       " 'have': 38,\n",
       " 'was': 36,\n",
       " 'in': 34,\n",
       " 'product': 33,\n",
       " 'that': 32,\n",
       " 'well': 31,\n",
       " 'headset': 31,\n",
       " 'quality': 31,\n",
       " 'sound': 27,\n",
       " 'so': 26,\n",
       " 'excellent': 26,\n",
       " 'price': 25,\n",
       " 'its': 24,\n",
       " 'has': 24,\n",
       " 'one': 23,\n",
       " 'are': 22,\n",
       " 'battery': 22,\n",
       " 'nice': 22,\n",
       " 'best': 21,\n",
       " 'use': 21,\n",
       " 'had': 21,\n",
       " 'but': 21,\n",
       " 'you': 21,\n",
       " 'love': 20,\n",
       " 'as': 20,\n",
       " 'recommend': 20,\n",
       " 'all': 20,\n",
       " 'than': 19,\n",
       " 'ive': 19,\n",
       " 'like': 18,\n",
       " 'case': 18,\n",
       " 'would': 17,\n",
       " 'from': 16,\n",
       " 'ear': 16,\n",
       " 'really': 15,\n",
       " 'not': 15,\n",
       " 'any': 15,\n",
       " 'easy': 14,\n",
       " 'comfortable': 14,\n",
       " 'your': 14,\n",
       " 'happy': 13,\n",
       " 'these': 13,\n",
       " 'better': 12,\n",
       " 'am': 12,\n",
       " 'im': 12,\n",
       " 'been': 12,\n",
       " 'just': 12,\n",
       " 'no': 12,\n",
       " 'up': 12,\n",
       " 'bluetooth': 12,\n",
       " 'fine': 12,\n",
       " 'new': 12,\n",
       " 'also': 11,\n",
       " 'be': 11,\n",
       " 'even': 11,\n",
       " 'time': 11,\n",
       " 'car': 11}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_75_words = pd.Series(' '.join(no_negative_reviews_df['Feedback']).lower().split()).value_counts()[:75]\n",
    "top_75_words.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Feature Engineering on the Third Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_amazon_keywords_list = ['great', 'very', 'good', 'works', 'well', 'quality', 'excellent',\n",
    "                        'nice', 'best', 'love', 'recommend', 'like', 'really', 'easy',\n",
    "                        'comfortable', 'happy', 'better', 'fine', 'new']\n",
    "\n",
    "for key in third_amazon_keywords_list:\n",
    "    third_amazon_df[str(key)] = third_amazon_df.Feedback.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Building the Training Data on the Third Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_amazon_data = third_amazon_df[third_amazon_keywords_list]\n",
    "third_amazon_target = third_amazon_df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Running the Model on the Third Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in the Amazon dataframe out of a total 1000 points : 289\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(third_amazon_data, third_amazon_target)\n",
    "\n",
    "y_pred = mnb.predict(third_amazon_data)\n",
    "\n",
    "print('Number of mislabeled points in the Amazon dataframe out of a total {} points : {}'.format(\n",
    "    third_amazon_data.shape[0],\n",
    "    (third_amazon_target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Third Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Success Rate Evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the third model is 71.1 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_score(third_amazon_target, y_pred)\n",
    "print('The accuracy of the third model is', accuracy_score(third_amazon_target, y_pred) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Confusion Matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[473,  27],\n",
       "       [262, 238]], dtype=int64)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(third_amazon_target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False Positive Count: 27.\n",
    "- False Negative Count: 262.\n",
    "- Sensitivity: the model correctly identified 47.6% of the positives.\n",
    "- Specificity: 94.6% of negatives were correctly identified by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Cross Validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77, 0.7 , 0.73, 0.71, 0.74, 0.63, 0.68, 0.67, 0.71, 0.64])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb, third_amazon_data, third_amazon_target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fourth Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Setting Up the Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_amazon_df = amazon_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Feature Engineering on the Fourth Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going remove neutral words (such as 'very') from the keywords list and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped words: very, works, well, quality, recommend, like, really, easy, comfortable, better and new.\n",
    "\n",
    "fourth_amazon_keywords_list = ['great', 'good', 'excellent', 'nice', 'best', 'love', 'happy', 'fine']\n",
    "\n",
    "for key in fourth_amazon_keywords_list:\n",
    "    fourth_amazon_df[str(key)] = fourth_amazon_df.Feedback.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Building the Training Data on the Fourth Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_amazon_data = fourth_amazon_df[fourth_amazon_keywords_list]\n",
    "fourth_amazon_target = fourth_amazon_df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Running the Model on the Fourth Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in the Amazon dataframe out of a total 1000 points : 357\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(fourth_amazon_data, fourth_amazon_target)\n",
    "\n",
    "y_pred = mnb.predict(fourth_amazon_data)\n",
    "\n",
    "print('Number of mislabeled points in the Amazon dataframe out of a total {} points : {}'.format(\n",
    "    fourth_amazon_data.shape[0],\n",
    "    (fourth_amazon_target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Fourth Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Success Rate Evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the fourth model is 64.3 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_score(fourth_amazon_target, y_pred)\n",
    "print('The accuracy of the fourth model is', accuracy_score(fourth_amazon_target, y_pred) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Confusion Matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[494,   6],\n",
       "       [351, 149]], dtype=int64)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(fourth_amazon_target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False Positive Count: six.\n",
    "- False Negative Count: 351.\n",
    "- Sensitivity: the model correctly identified 29.8% of the positives.\n",
    "- Specificity: 98.8% of negatives were correctly identified by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Cross Validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69, 0.66, 0.66, 0.64, 0.63, 0.61, 0.61, 0.59, 0.59, 0.57])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb, amazon_data, fourth_amazon_target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fifth Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Setting Up the Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_amazon_df = amazon_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Feature Engineering on the Fifth Dataframe_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of experimenting and learning, I'm going to drop all the keywords but one and see what happens with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_amazon_keywords_list = ['great']\n",
    "\n",
    "for key in fifth_amazon_keywords_list:\n",
    "    fifth_amazon_df[str(key)] = fifth_amazon_df.Feedback.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Building the Training Data on the Fifth Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_amazon_data = fifth_amazon_df[fifth_amazon_keywords_list]\n",
    "fifth_amazon_target = fifth_amazon_df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Running the Model on the Fifth Amazon Dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points in the Amazon dataframe out of a total 1000 points : 500\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(fifth_amazon_data, fifth_amazon_target)\n",
    "\n",
    "y_pred = mnb.predict(fifth_amazon_data)\n",
    "\n",
    "print('Number of mislabeled points in the Amazon dataframe out of a total {} points : {}'.format(\n",
    "    fifth_amazon_data.shape[0],\n",
    "    (fifth_amazon_target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Fifth Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Success Rate Evaluation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the fifth model is 50.0 %\n"
     ]
    }
   ],
   "source": [
    "accuracy_score(fifth_amazon_target, y_pred)\n",
    "print('The accuracy of the fifth model is', accuracy_score(fifth_amazon_target, y_pred) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Confusion Matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[500,   0],\n",
       "       [500,   0]], dtype=int64)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(fifth_amazon_target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False Positive Count: 0.\n",
    "- False Negative Count: 500.\n",
    "- Sensitivity: the model correctly identified 0% of the positives.\n",
    "- Specificity: 100% of negatives were correctly identified by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Cross Validation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69, 0.66, 0.66, 0.64, 0.63, 0.61, 0.61, 0.59, 0.59, 0.57])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb, amazon_data, fifth_amazon_target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, here's a summary of each tables' evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>One</th>\n",
       "      <th>Two</th>\n",
       "      <th>Three</th>\n",
       "      <th>Four</th>\n",
       "      <th>Five</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False Positives</td>\n",
       "      <td>6.000</td>\n",
       "      <td>9.000</td>\n",
       "      <td>27.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False Negatives</td>\n",
       "      <td>370.000</td>\n",
       "      <td>310.000</td>\n",
       "      <td>262.000</td>\n",
       "      <td>351.000</td>\n",
       "      <td>500.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cross Validation</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Metric      One      Two    Three     Four    Five\n",
       "0          Accuracy    0.624    0.681    0.711    0.643    0.50\n",
       "1   False Positives    6.000    9.000   27.000    6.000    0.00\n",
       "2   False Negatives  370.000  310.000  262.000  351.000  500.00\n",
       "3       Sensitivity    0.260    0.380    0.476    0.298    0.00\n",
       "4       Specificity    0.989    0.982    0.946    0.988    1.00\n",
       "5  Cross Validation    0.630    0.670    0.700    0.630    0.63"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = pd.read_excel('18.10 Challenge Model Data.xlsx', delimiter='\\t')\n",
    "model_df.columns = ['Metric', 'One', 'Two', 'Three', 'Four', 'Five']\n",
    "\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model three seemed to perform the best because it had the highest accuracy and cross validation scores, plus it had the least false negatives. \n",
    "\n",
    "Although it did have more false positives than the other models and its sensitivity was greater than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
